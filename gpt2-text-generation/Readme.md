# GPT-2 Text Generation with Hugging Face ğŸ¤–ğŸ“

A simple script that loads a pre-trained GPT-2 model and generates text from a prompt using top-k sampling with different temperatures.

---

## ğŸ“Œ Objective

Demonstrate:
- Loading a pre-trained transformer (`gpt2`)
- Text generation with Hugging Face's `transformers` library
- Sampling with `top_k=50` and varying `temperature` (0.7 and 1.0)

---

## ğŸ› ï¸ Requirements

Install dependencies:

```bash
pip install torch transformers

â–¶ï¸ How to Run

python generate.py

ğŸ“¤ Output

A file named generated_outputs.txt will be created with two generated samples:

=== Output with temperature=0.7 ===
Once upon a time...

=== Output with temperature=1.0 ===
Once upon a time...

ğŸ§  Sampling Settings
Parameter	Value
Model	gpt2
Max Tokens	50
Top-k	50
Temperatures	0.7, 1.0

ğŸ“ Project Files

gpt2-text-generation/
â”œâ”€â”€ generate.py              # Main script
â”œâ”€â”€ generated_outputs.txt    # Generated samples
â””â”€â”€ requirements.txt         # Dependencies (optional)
